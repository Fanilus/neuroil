{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mood.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVzQGixJbr6K"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.datasets import imdb\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU-coHW1dJO1"
      },
      "source": [
        "Поскольку мы не хотим иметь данные обучения и тестирования в пропорции 50/50, мы сразу же объединим эти данные после загрузки для последующего разделения в пропорции 80/20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyfnJN0bb9gv",
        "outputId": "85fc0666-cf4d-46be-9f11-d7c20d0e2e0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\n",
        "data = np.concatenate((training_data, testing_data), axis=0)\n",
        "targets = np.concatenate((training_targets, testing_targets), axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOceLzwkdEg6"
      },
      "source": [
        "Нужно векторизовать каждый обзор и заполнить его нулями, чтобы вектор содержал ровно 10 000 чисел. Это означает, что каждый обзор, который короче 10 000 слов, мы заполняем нулями. Это делается потому, что самый большой обзор имеет почти такой же размер, а каждый элемент входных данных нашей нейронной сети должен иметь одинаковый размер. Также нужно выполнить преобразование переменных в тип float"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cIwV1SAcHB1"
      },
      "source": [
        "def vectorize(sequences, dimension = 10000):\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] = 1\n",
        "  return results\n",
        " \n",
        "data = vectorize(data)\n",
        "targets = np.array(targets).astype(\"float32\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGsv8Yh7d1C3"
      },
      "source": [
        "Разделим датасет на обучающий и тестировочный наборы. Обучающий набор будет состоять из 40 000 обзоров, тестировочный — из 10 000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7rqNjTGd4y8"
      },
      "source": [
        "test_x = data[:10000]\n",
        "test_y = targets[:10000]\n",
        "train_x = data[10000:]\n",
        "train_y = targets[10000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vgzi9ek2fIJV",
        "outputId": "b473588e-2479-4190-e642-42a7d1c174dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "model = keras.Sequential()\n",
        "model.add(layers.Dense(50, activation=\"relu\", input_shape=(10000, )))\n",
        "\n",
        "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
        "model.add(layers.Dense(50, activation=\"relu\"))\n",
        "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
        "model.add(layers.Dense(50, activation=\"relu\"))\n",
        "\n",
        "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 50)                500050    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 505,201\n",
            "Trainable params: 505,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgY1jNDtk3rO"
      },
      "source": [
        "В качестве функции потерь используем бинарную кросс-энтропию (так как мы работаем с бинарной классификацией), в качестве метрики оценки — точность"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt0wcM8WkmKO"
      },
      "source": [
        "model.compile(\n",
        "    optimizer = \"adam\",\n",
        "    loss = \"binary_crossentropy\",\n",
        "    metrics = [\"accuracy\"]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odcq3MAbky3-"
      },
      "source": [
        "Размер партии 500 и только 2 эпохи, поскольку модель начинает переобучаться, если тренировать ее дольше"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d29L0t0P2mr4",
        "outputId": "a62cfb6a-0a25-4f81-eaef-01114179a94e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(train_x.shape)\n",
        "print(test_y[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40000, 10000)\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JGEtOstk8AG",
        "outputId": "ef171e86-cd8f-4a29-c6ca-ce6478aa6053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "results = model.fit(\n",
        "    train_x, train_y,\n",
        "    epochs= 2,\n",
        "    batch_size = 500,\n",
        "    validation_data = (test_x, test_y)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.4098 - accuracy: 0.8174 - val_loss: 0.2589 - val_accuracy: 0.8968\n",
            "Epoch 2/2\n",
            "80/80 [==============================] - 3s 39ms/step - loss: 0.2147 - accuracy: 0.9174 - val_loss: 0.2610 - val_accuracy: 0.8923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBqM9TdiljOl",
        "outputId": "cf7b0bf3-f7ee-4eba-ae48-522481519561",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Test-Accuracy:\", np.mean(results.history[\"val_accuracy\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test-Accuracy: 0.8945499956607819\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}